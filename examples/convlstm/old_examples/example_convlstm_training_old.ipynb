{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "699d8090-28bf-4d29-8a05-1ec8b6a9b9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional\n",
    "from torch.autograd import Variable\n",
    "import sys\n",
    "# import convlstm\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import convlstm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3644bd2-d85e-4ef8-bb6a-98240831bac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_channels = 10 # number of input channels e.g. concentration heatmap, current, wind curl, etc.\n",
    "hidden_channels = [10, 5, 1] # the last digit is the output channel\n",
    "kernel_size = 3\n",
    "batch_size = 1\n",
    "learning_rate = 0.01\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f727596e-c692-4a23-a63a-545a24941848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version 1.8.1\n",
      "Device to be used for computation: cpu\n"
     ]
    }
   ],
   "source": [
    "print (\"Pytorch version {}\".format(torch.__version__))\n",
    "# check if CUDA is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# use GPU if possible\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device to be used for computation: {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5db0fdb5-bdda-43fe-b791-411a7afcb77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvLSTM(\n",
      "  (cell0): ConvLSTMCell(\n",
      "    (Wxi): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Whi): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (Wxf): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Whf): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (Wxc): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Whc): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (Wxo): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Who): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (cell1): ConvLSTMCell(\n",
      "    (Wxi): Conv2d(10, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Whi): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (Wxf): Conv2d(10, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Whf): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (Wxc): Conv2d(10, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Whc): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (Wxo): Conv2d(10, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Who): Conv2d(5, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (cell2): ConvLSTMCell(\n",
      "    (Wxi): Conv2d(5, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Whi): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (Wxf): Conv2d(5, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Whf): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (Wxc): Conv2d(5, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Whc): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (Wxo): Conv2d(5, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (Who): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      ")\n",
      "MSELoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 0.01\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/export/lv4/user/jfajardourbina/programs/miniconda3/envs/phd_parcelsv221/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "# initialize our model\n",
    "model = convlstm.ConvLSTM(input_channels, hidden_channels, kernel_size).cpu()\n",
    "# choose loss function\n",
    "loss_fn = torch.nn.MSELoss(size_average=True)\n",
    "# choose optimizer\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# check the model / loss function and optimizer\n",
    "print(model)\n",
    "print(loss_fn)\n",
    "print(optimiser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "554702ac-3faa-4624-ae09-77cdc702fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vector = Variable(torch.randn(1, 10, 5, 1)).to(device)\n",
    "target_vector = Variable(torch.randn(1, 10, 5, 1)).double().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebebe9a-2c47-489e-bc72-552abefbfa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de33a31-dd97-4057-98a3-9f6facf7613c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = np.zeros(num_epochs)\n",
    "    # loop of epoch\n",
    "    for t in range(num_epochs):\n",
    "        # Clear stored gradient\n",
    "        model.zero_grad()\n",
    "        # loop of timestep\n",
    "        for timestep in range(sequence_len - cross_valid_year*12*4 - test_year*12*4):\n",
    "            # hidden state re-initialized inside the model when timestep=0\n",
    "            #################################################################################\n",
    "            ########          create input tensor with multi-input dimension         ########\n",
    "            #################################################################################\n",
    "            # create variables\n",
    "            x_input = np.stack((sic_exp_norm[timestep,:,:],\n",
    "                                ohc_exp_norm[timestep,:,:],\n",
    "                                t2m_exp_norm[timestep,:,:],\n",
    "                                slp_exp_norm[timestep,:,:],\n",
    "                                z500_exp_norm[timestep,:,:],\n",
    "                                z850_exp_norm[timestep,:,:],\n",
    "                                u10m_exp_norm[timestep,:,:],\n",
    "                                v10m_exp_norm[timestep,:,:],\n",
    "                                sflux_exp_norm[timestep,:,:],\n",
    "                                month_exp[timestep,:,:])) #vstack,hstack,dstack\n",
    "            x_var = torch.autograd.Variable(torch.Tensor(x_input).view(-1,input_channels,height,width)).cuda()\n",
    "            #################################################################################\n",
    "            ########       create training tensor with multi-input dimension         ########\n",
    "            #################################################################################\n",
    "            y_train_stack = sic_exp_norm[timestep+1,:,:] #vstack,hstack,dstack\n",
    "            y_var = torch.autograd.Variable(torch.Tensor(y_train_stack).view(-1,hidden_channels[-1],height,width)).cuda()\n",
    "            #################################################################################   \n",
    "            # Forward pass\n",
    "            y_pred, _ = model(x_var, timestep)\n",
    "            # choose training data\n",
    "            y_train = y_var        \n",
    "            # torch.nn.functional.mse_loss(y_pred, y_train) can work with (scalar,vector) & (vector,vector)\n",
    "            # Please Make Sure y_pred & y_train have the same dimension\n",
    "            # accumulate loss\n",
    "            if timestep == 0:\n",
    "                loss = loss_fn(y_pred, y_train)\n",
    "            else:\n",
    "                loss += loss_fn(y_pred, y_train)\n",
    "            #print (timestep)\n",
    "        #print(y_pred.shape)\n",
    "        #print(y_train.shape)\n",
    "        # print loss at certain iteration\n",
    "        if t % 5 == 0:\n",
    "            print(\"Epoch \", t, \"MSE: \", loss.item())\n",
    "            #print(y_pred)\n",
    "            # gradient check\n",
    "            # Gradcheck requires double precision numbers to run\n",
    "            #res = torch.autograd.gradcheck(loss_fn, (y_pred.double(), y_train.double()), eps=1e-6, raise_exception=True)\n",
    "            #print(res)\n",
    "        hist[t] = loss.item()\n",
    "\n",
    "        # Zero out gradient, else they will accumulate between epochs\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimiser.step()\n",
    "        \n",
    "    torch.save(model, os.path.join(output_path,'convlstm_monteCarlo.pkl')) # save lstm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6f4aeb3-3ada-459e-82c3-becbcf827822",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce519709-b677-4ee0-ae41-47c31cdb0b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time series before test data\n",
    "    pred_base_sic = sic_exp_norm[:-test_year*12*4,:,:]\n",
    "    # predict x steps ahead\n",
    "    step_lead = 16 # unit week\n",
    "    # create a matrix for the prediction\n",
    "    lead_pred_sic = np.zeros((test_year*12*4,step_lead,height,width),dtype=float) # dim [predict time, lead time, lat, lon]\n",
    "    # start the prediction loop\n",
    "    for step in range(test_year*12*4):\n",
    "        # Clear stored gradient\n",
    "        model.zero_grad()\n",
    "        # Don't do this if you want your LSTM to be stateful\n",
    "        # Otherwise the hidden state should be cleaned up at each time step for prediction (we don't clear hidden state in our forward function)\n",
    "        # see example from (https://github.com/pytorch/examples/blob/master/time_sequence_prediction/train.py)\n",
    "        # model.hidden = model.init_hidden()\n",
    "        # based on the design of this module, the hidden states and cell states are initialized when the module is called.\n",
    "        for i in np.arange(1,sequence_len-test_year*12*4 + step + step_lead,1): # here i is actually the time step (index) of prediction, we use var[:i] to predict var[i]\n",
    "            #############################################################################\n",
    "            ###############           before time of prediction           ###############\n",
    "            #############################################################################\n",
    "            if i <= (sequence_len-test_year*12*4 + step):\n",
    "                # create variables\n",
    "                x_input = np.stack((sic_exp_norm[i-1,:,:],\n",
    "                                    ohc_exp_norm[i-1,:,:],\n",
    "                                    t2m_exp_norm[i-1,:,:],\n",
    "                                    slp_exp_norm[i-1,:,:],\n",
    "                                    z500_exp_norm[i-1,:,:],\n",
    "                                    z850_exp_norm[i-1,:,:],\n",
    "                                    u10m_exp_norm[i-1,:,:],\n",
    "                                    v10m_exp_norm[i-1,:,:],\n",
    "                                    sflux_exp_norm[i-1,:,:],\n",
    "                                    month_exp[i-1,:,:])) #vstack,hstack,dstack\n",
    "                x_var_pred = torch.autograd.Variable(torch.Tensor(x_input).view(-1,input_channels,height,width),\n",
    "                                                     requires_grad=False).cuda()\n",
    "                # make prediction\n",
    "                last_pred, _ = model(x_var_pred, i-1)\n",
    "                # record the real prediction after the time of prediction\n",
    "                if i == (sequence_len-test_year*12*4 + step):\n",
    "                    lead = 0\n",
    "                    # GPU data should be transferred to CPU\n",
    "                    lead_pred_sic[step,0,:,:] = last_pred[0,0,:,:].cpu().data.numpy()\n",
    "            #############################################################################\n",
    "            ###############            after time of prediction           ###############\n",
    "            #############################################################################\n",
    "            else:\n",
    "                lead += 1\n",
    "                # prepare predictor\n",
    "                if i <= sequence_len:\n",
    "                    # use the predicted data to make new prediction\n",
    "                    x_input = np.stack((lead_pred_sic[step,i-(sequence_len-test_year*12*4 + step +1),:,:],\n",
    "                                        ohc_exp_norm[i-1,:,:],\n",
    "                                        t2m_exp_norm[i-1,:,:],\n",
    "                                        slp_exp_norm[i-1,:,:],\n",
    "                                        z500_exp_norm[i-1,:,:],\n",
    "                                        z850_exp_norm[i-1,:,:],\n",
    "                                        u10m_exp_norm[i-1,:,:],\n",
    "                                        v10m_exp_norm[i-1,:,:],\n",
    "                                        sflux_exp_norm[i-1,:,:],\n",
    "                                        month_exp[i-1,:,:])) #vstack,hstack,dstack\n",
    "                else: # choice_exp_norm out of range, use the last value\n",
    "                    x_input = np.stack((lead_pred_sic[step,i-(sequence_len-test_year*12*4 + step +1),:,:],\n",
    "                                        ohc_exp_norm[-1,:,:],\n",
    "                                        t2m_exp_norm[-1,:,:],\n",
    "                                        slp_exp_norm[-1,:,:],\n",
    "                                        z500_exp_norm[-1,:,:],\n",
    "                                        z850_exp_norm[-1,:,:],\n",
    "                                        u10m_exp_norm[-1,:,:],\n",
    "                                        v10m_exp_norm[-1,:,:],\n",
    "                                        sflux_exp_norm[-1,:,:],\n",
    "                                        month_exp[i-1,:,:])) #vstack,hstack,dstack                    \n",
    "                x_var_pred = torch.autograd.Variable(torch.Tensor(x_input).view(-1,input_channels,height,width),\n",
    "                                                     requires_grad=False).cuda()        \n",
    "                # make prediction\n",
    "                last_pred, _ = model(x_var_pred, i-1)\n",
    "                # record the prediction\n",
    "                lead_pred_sic[step,lead,:,:] = last_pred[0,0,:,:].cpu().data.numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
