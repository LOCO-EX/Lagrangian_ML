{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Example script: training and predicting with ConvLSTM\n",
    "Author          : SSI project team Wadden Sea <br>\n",
    "First Built     : 2021.08.01 <br>\n",
    "Last Update     : 2021.08.02 <br>\n",
    "Description     : This notebook serves as an example of training and predicting with\n",
    "                  Convolutional Long-Short Term Memeory Neural Network (ConvLSTM). <br>\n",
    "Dependency      : os, numpy, pytorch <br>\n",
    "Return Values   : time series / array <br>\n",
    "Caveat!         : This module performs many-to-one prediction! It supports CUDA. <br>\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional\n",
    "from torch.autograd import Variable\n",
    "# import convlstm\n",
    "sys.path.append(\"../src\")\n",
    "import convlstm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyper-parameter of neural network"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_channels = 10 # number of input channels e.g. concentration heatmap, current, wind curl, etc.\n",
    "hidden_channels = [10, 5, 1] # the last digit is the output channel\n",
    "kernel_size = 3\n",
    "batch_size = 1\n",
    "learning_rate = 0.01\n",
    "num_epochs = 20"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hardware info and version of pytorch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print (\"Pytorch version {}\".format(torch.__version__))\n",
    "# check if CUDA is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# use GPU if possible\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device to be used for computation: {}\".format(device))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initialize model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# initialize our model\n",
    "model = convlstm.ConvLSTM(input_channels, hidden_channels, kernel_size).cuda()\n",
    "# choose loss function\n",
    "loss_fn = torch.nn.MSELoss(size_average=True)\n",
    "# choose optimizer\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# check the model / loss function and optimizer\n",
    "print(model)\n",
    "print(loss_fn)\n",
    "print(optimiser)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create dummy data for testing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_vector = Variable(torch.randn(1, 10, 5, 1)).to(device)\n",
    "target_vector = Variable(torch.randn(1, 10, 5, 1)).double().to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "    hist = np.zeros(num_epochs)\n",
    "    # loop of epoch\n",
    "    for t in range(num_epochs):\n",
    "        # Clear stored gradient\n",
    "        model.zero_grad()\n",
    "        # loop of timestep\n",
    "        for timestep in range(sequence_len - cross_valid_year*12*4 - test_year*12*4):\n",
    "            # hidden state re-initialized inside the model when timestep=0\n",
    "            #################################################################################\n",
    "            ########          create input tensor with multi-input dimension         ########\n",
    "            #################################################################################\n",
    "            # create variables\n",
    "            x_input = np.stack((sic_exp_norm[timestep,:,:],\n",
    "                                ohc_exp_norm[timestep,:,:],\n",
    "                                t2m_exp_norm[timestep,:,:],\n",
    "                                slp_exp_norm[timestep,:,:],\n",
    "                                z500_exp_norm[timestep,:,:],\n",
    "                                z850_exp_norm[timestep,:,:],\n",
    "                                u10m_exp_norm[timestep,:,:],\n",
    "                                v10m_exp_norm[timestep,:,:],\n",
    "                                sflux_exp_norm[timestep,:,:],\n",
    "                                month_exp[timestep,:,:])) #vstack,hstack,dstack\n",
    "            x_var = torch.autograd.Variable(torch.Tensor(x_input).view(-1,input_channels,height,width)).cuda()\n",
    "            #################################################################################\n",
    "            ########       create training tensor with multi-input dimension         ########\n",
    "            #################################################################################\n",
    "            y_train_stack = sic_exp_norm[timestep+1,:,:] #vstack,hstack,dstack\n",
    "            y_var = torch.autograd.Variable(torch.Tensor(y_train_stack).view(-1,hidden_channels[-1],height,width)).cuda()\n",
    "            #################################################################################   \n",
    "            # Forward pass\n",
    "            y_pred, _ = model(x_var, timestep)\n",
    "            # choose training data\n",
    "            y_train = y_var        \n",
    "            # torch.nn.functional.mse_loss(y_pred, y_train) can work with (scalar,vector) & (vector,vector)\n",
    "            # Please Make Sure y_pred & y_train have the same dimension\n",
    "            # accumulate loss\n",
    "            if timestep == 0:\n",
    "                loss = loss_fn(y_pred, y_train)\n",
    "            else:\n",
    "                loss += loss_fn(y_pred, y_train)\n",
    "            #print (timestep)\n",
    "        #print(y_pred.shape)\n",
    "        #print(y_train.shape)\n",
    "        # print loss at certain iteration\n",
    "        if t % 5 == 0:\n",
    "            print(\"Epoch \", t, \"MSE: \", loss.item())\n",
    "            #print(y_pred)\n",
    "            # gradient check\n",
    "            # Gradcheck requires double precision numbers to run\n",
    "            #res = torch.autograd.gradcheck(loss_fn, (y_pred.double(), y_train.double()), eps=1e-6, raise_exception=True)\n",
    "            #print(res)\n",
    "        hist[t] = loss.item()\n",
    "\n",
    "        # Zero out gradient, else they will accumulate between epochs\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters\n",
    "        optimiser.step()\n",
    "        \n",
    "    torch.save(model, os.path.join(output_path,'convlstm_monteCarlo.pkl')) # save lstm model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "    # time series before test data\n",
    "    pred_base_sic = sic_exp_norm[:-test_year*12*4,:,:]\n",
    "    # predict x steps ahead\n",
    "    step_lead = 16 # unit week\n",
    "    # create a matrix for the prediction\n",
    "    lead_pred_sic = np.zeros((test_year*12*4,step_lead,height,width),dtype=float) # dim [predict time, lead time, lat, lon]\n",
    "    # start the prediction loop\n",
    "    for step in range(test_year*12*4):\n",
    "        # Clear stored gradient\n",
    "        model.zero_grad()\n",
    "        # Don't do this if you want your LSTM to be stateful\n",
    "        # Otherwise the hidden state should be cleaned up at each time step for prediction (we don't clear hidden state in our forward function)\n",
    "        # see example from (https://github.com/pytorch/examples/blob/master/time_sequence_prediction/train.py)\n",
    "        # model.hidden = model.init_hidden()\n",
    "        # based on the design of this module, the hidden states and cell states are initialized when the module is called.\n",
    "        for i in np.arange(1,sequence_len-test_year*12*4 + step + step_lead,1): # here i is actually the time step (index) of prediction, we use var[:i] to predict var[i]\n",
    "            #############################################################################\n",
    "            ###############           before time of prediction           ###############\n",
    "            #############################################################################\n",
    "            if i <= (sequence_len-test_year*12*4 + step):\n",
    "                # create variables\n",
    "                x_input = np.stack((sic_exp_norm[i-1,:,:],\n",
    "                                    ohc_exp_norm[i-1,:,:],\n",
    "                                    t2m_exp_norm[i-1,:,:],\n",
    "                                    slp_exp_norm[i-1,:,:],\n",
    "                                    z500_exp_norm[i-1,:,:],\n",
    "                                    z850_exp_norm[i-1,:,:],\n",
    "                                    u10m_exp_norm[i-1,:,:],\n",
    "                                    v10m_exp_norm[i-1,:,:],\n",
    "                                    sflux_exp_norm[i-1,:,:],\n",
    "                                    month_exp[i-1,:,:])) #vstack,hstack,dstack\n",
    "                x_var_pred = torch.autograd.Variable(torch.Tensor(x_input).view(-1,input_channels,height,width),\n",
    "                                                     requires_grad=False).cuda()\n",
    "                # make prediction\n",
    "                last_pred, _ = model(x_var_pred, i-1)\n",
    "                # record the real prediction after the time of prediction\n",
    "                if i == (sequence_len-test_year*12*4 + step):\n",
    "                    lead = 0\n",
    "                    # GPU data should be transferred to CPU\n",
    "                    lead_pred_sic[step,0,:,:] = last_pred[0,0,:,:].cpu().data.numpy()\n",
    "            #############################################################################\n",
    "            ###############            after time of prediction           ###############\n",
    "            #############################################################################\n",
    "            else:\n",
    "                lead += 1\n",
    "                # prepare predictor\n",
    "                if i <= sequence_len:\n",
    "                    # use the predicted data to make new prediction\n",
    "                    x_input = np.stack((lead_pred_sic[step,i-(sequence_len-test_year*12*4 + step +1),:,:],\n",
    "                                        ohc_exp_norm[i-1,:,:],\n",
    "                                        t2m_exp_norm[i-1,:,:],\n",
    "                                        slp_exp_norm[i-1,:,:],\n",
    "                                        z500_exp_norm[i-1,:,:],\n",
    "                                        z850_exp_norm[i-1,:,:],\n",
    "                                        u10m_exp_norm[i-1,:,:],\n",
    "                                        v10m_exp_norm[i-1,:,:],\n",
    "                                        sflux_exp_norm[i-1,:,:],\n",
    "                                        month_exp[i-1,:,:])) #vstack,hstack,dstack\n",
    "                else: # choice_exp_norm out of range, use the last value\n",
    "                    x_input = np.stack((lead_pred_sic[step,i-(sequence_len-test_year*12*4 + step +1),:,:],\n",
    "                                        ohc_exp_norm[-1,:,:],\n",
    "                                        t2m_exp_norm[-1,:,:],\n",
    "                                        slp_exp_norm[-1,:,:],\n",
    "                                        z500_exp_norm[-1,:,:],\n",
    "                                        z850_exp_norm[-1,:,:],\n",
    "                                        u10m_exp_norm[-1,:,:],\n",
    "                                        v10m_exp_norm[-1,:,:],\n",
    "                                        sflux_exp_norm[-1,:,:],\n",
    "                                        month_exp[i-1,:,:])) #vstack,hstack,dstack                    \n",
    "                x_var_pred = torch.autograd.Variable(torch.Tensor(x_input).view(-1,input_channels,height,width),\n",
    "                                                     requires_grad=False).cuda()        \n",
    "                # make prediction\n",
    "                last_pred, _ = model(x_var_pred, i-1)\n",
    "                # record the prediction\n",
    "                lead_pred_sic[step,lead,:,:] = last_pred[0,0,:,:].cpu().data.numpy()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "90e02b2587fbb2ca467fc85381f5522fddb4a9e5fbb8605712260c849ecf752b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}